# Dissecting Neural ODEs
###### tags: `Paper Summary`,`Deep Learning`,`NODEs`
---
Reference: Massaroli, S., Poli, M., Park, J., Yamashita, A., & Asama, H. (2020). [Dissecting neural odes.](https://arxiv.org/abs/2002.08071) *arXiv preprint arXiv:2002.08071.*
Code: https://github.com/DiffEqML/diffeqml-research/tree/master/dissecting-neural-odes

---


# 1. Introduction

### Theoretic Neural ODE Formulation

$$
\begin{cases}\ \dot{\mathbf{ z}}=f_{\theta (s)}(s,\mathbf{x},\mathbf{z}(s))\\ \mathbf{z}(0)=h_x(\mathbf{x})\\\hat{\mathbf{y}}(s)=h_y(\mathbf{z}(s))\end{cases}, s\in \mathcal{S} \;\;(1)
$$


While dependance on depth-variable has beeen considered in the original formulation (Vanilla Neural ODEs), **depth-variance** has been overlooked.

- **Depth-variance**
    
    Discuss the subtleties involved, uncovering a formal optimization problem ==in functional space== as the price to pay for true depth–variance.
    
    Two novel variants of Neural ODEs: 
    
    - Spectral Discretization—**GalNODE**: involving a loss distributed on the depth–domain :arrow_right: generalized version of the adjoint
    - Depth Discretization—Piecewise–constant model
- **Augmentation Strategies**
    
    ANODEs(0-augmentation) vs alternative: relies on different choices of $h_x$ ( input-layer and higher-order augmentation )
    
    More effective in terms of performance and parameter efficientcy
    
- **Beyond augmentation: data–control and adaptive–depth**
    
    Depth–varying vector ﬁelds alone are sufﬁcient in dimensions greater than one.
    
    - **Data-control** - conditioned $f_{\theta(s)}$, learn a family of vector ﬁelds instead of a single one
    - **Depth–adaptation** - the integration bound is itself determined by an auxiliary neural network
    
::: danger
⚠️ Input networks $h_x$ of the multilayer, nonlinear type, as these can make Neural ODE ﬂows *superﬂuous*.
:::
    
    

# 2. Continuous-Depth Models

## A general formulation of Neural ODEs

Input-output data :  $\{(\mathbf{x}_k, \mathbf{y}_k)|k\in\mathcal{K}\}$  ($\mathcal{K}$ is a linearly-ordered finite subset of $\mathbb{N}$)

Solving initial value problem(IVP) $\begin{cases}\ \dot{\mathbf{ z}}=f_{\theta (s)}(s,\mathbf{x},\mathbf{z}(s))\\ \mathbf{z}(0)=h_x(\mathbf{x})\\\hat{\mathbf{y}}(s)=h_y(\mathbf{z}(s))\end{cases}, s\in \mathcal{S} \;\;(1)$:

$$
 \hat{\mathbf{y}}(S)=h_y(h_x(\mathbf{x})+\int_\mathcal{S}f_{\theta(\tau)}(\tau, \mathbf{x},\mathbf{z}(\tau))d\tau)
$$

Degree of freedom: the choice of  $\theta$ inside $\mathcal{W}$, where $\mathcal{W}$ is a class of functions $\mathcal{S}\to\mathbb{R}^{n_\theta}$ 

## Well–posedness

If $f_{\theta(s)}$ is Lipschitz, for each $\mathbf{x}_k$ the IVP $(1)$ admits a unique solution $\mathbf{z}$ defined in the whole $\mathcal{S}$.

If this is the case,$\exists\text{ a mapping }\phi$ from $\mathcal{W}\times\mathbb{R}^{n_x}$ to the (absolutely continuous) function space $\mathcal{S}\to\mathbb{R}^{n_z}$ s.t. $\mathbf{z}_k\equiv \phi(\mathbf{x}_k, \theta)$ satisfies the ODE in (1).

Then, $\forall k\in\mathcal{K}, \text{the map }(\theta,\mathbf{x}_k, s)\mapsto\gamma(\theta,\mathbf{x_k},s)\equiv h_y(\phi_s(\theta,\mathbf{x}_k))$ satisfies $\hat{\mathbf{y}}=\gamma(\theta,\mathbf{x}_k, s)$

For compactness, we denote $\phi(\theta,\mathbf{x}_k)(s)$ by $\phi_s(\theta,\mathbf{x}_k)$, $\forall s\in\mathcal{S}$
![well-posedness](https://i.imgur.com/hfMX0da.png)



## Training: Optimal Control

- Vanilla Neual ODEs: $\mathcal{W}$ is the space of constant function. Only considering *terminal loss function* depend on the terminla state $\mathbf{z}(S)$.

Loss function distributed on $\mathcal{S}$. e.g.

$$
\ell\equiv L(\mathbf{z}(S))+\int_\mathcal{S}l(\tau,\mathbf{z}(\tau))d\tau \ \ \ (2)
$$

Training will be the optimal control problem:

$$
\min_{\theta\in\mathcal{W}}\frac{1}{|\mathcal{K}|}\sum_{k\in\mathcal{K}}\ell_k, \text{ subject to } \begin{cases}\ \ \ \ \  \dot{\mathbf{ z}}=f_{\theta (s)}(s,\mathbf{x}_k,\mathbf{z}(s))\\ \mathbf{z}(0)=h_x(\mathbf{x}_k)\\\hat{\mathbf{y}}(s)=h_y(\mathbf{z}(s))\end{cases}, s\in\mathcal{S}, \forall k \in \mathcal{K}\ \ \ (3)
$$

### **Proposition 1** (Generalized Adjoint Method)

If $\theta$ is constant, the gradients can be computed with $\mathcal{O}(1)$ memory efficientcy by the Generalized Adjoint Method: 

$$
\frac{d\ell}{d\theta}=\int_\mathcal{S}\mathbf{a}^T(\tau)\frac{\partial f_\theta}{\partial \theta}d\tau\text{, where }\mathbf{a}(s)\text{ satisfies }\begin{cases}\dot{\mathbf{a}}^T(s)=-\mathbf{a}^T\frac{\partial f_\theta}{\partial \mathbf{z}}- \frac{\partial l}{\partial \mathbf{z}}\\\mathbf{a}^T(S)=\frac{\partial L}{\partial \mathbf{z}(S)}\end{cases}
$$

# 3. Depth-Variance: Infinite Dimension for Infinite Layers

- Vanilla Neural ODEs cannot be fully considered the *deep limit* of ResNets.
- The depth variable $s$ enters the dynamic  **by itself** , in Vanilla Neural ODEs: $\dot{\mathbf{z}}=f_\theta(s,\mathbf{z}(s))$ , vs **mapping** $s \mapsto\theta(s)$

## Gradien Descent in Functional Space

When the model parameters are depth-varting , $\theta:\mathcal{S}\to\mathbb{R}^{n_\theta}$, the optimization problem (3) should be solved by iterating a gradient descent algorithm in a functional space, e.g. $\theta_{k+1}(s)=\theta_k(s)-\frac{\delta\ell_k}{\delta\theta(s)}$, once the Gateaux deriative is computed.
:::info
[Gateaux deriative](https://en.wikipedia.org/wiki/Gateaux_derivative): generalization of the concept of directional derivative in differential calculus
:::


## Theorem 1 (Infinite-Dimensional Gradients)

Consider the loss function $\ell\equiv L(\mathbf{z}(S))+\int_\mathcal{S}l(\tau,\mathbf{z}(\tau))d\tau \ \ \ (2)$ and let $\theta(s)\in\mathcal{W}\equiv\mathbb{L}_2(\mathcal{S}\to\mathbb{R}^{n_\theta})$, Then,

$\text{sentisivity of }\ell\text{ with respect to }\theta(s)$(i.e. directional deriative in functoinal space) is 

$$
\frac{\delta \ell}{\delta \theta(s)}=\mathbf{a}^T(\tau)\frac{\partial f_{\theta(s)}}{\partial \theta(s)}\text{, where}\;\mathbf{a}(s)\text{ satisfies }\begin{cases}\dot {\mathbf{a}}^T(s)=-\mathbf{a}^T(s)\frac{\partial  f_{\theta(s)}}{\partial \mathbf{z}}-\frac{\partial l}{\partial \mathbf{z}}\\ \mathbf{a}^T(S)=\frac{\partial L}{\partial \mathbf{z}(S)}\end{cases}
$$

:::warning
⚠️ The implementation of Theorem 1 still requires choosing a *finite dimensional approximation* of the solution.
:::


## Spectral Discretization: Galerkin Neural ODEs

Expand $\theta(s)$ on a complete orthogonal basis$\{\psi_j:j=1,2,\dots\}$ of a predetermined subspace of $\mathbb{L}_2(\mathcal{S}\to\mathbb{R}^{n_\theta})$ and truncate the series to the $m^{}$-th term:

$$
\theta(s) = \sum^m_{j=1}\alpha_j\odot\psi_j(s)
$$

Optimize the parameters $\alpha=(\alpha_1,\dots,\alpha_m)\in\mathbb{R}^{mn_\theta}$, whose gradient can be computed as:

$$
\frac{d\ell}{d\alpha}=\int_\mathcal{S}\mathbf{a}^T(\tau)\frac{\partial f_{\theta(s)}}{\partial \theta(s)}\psi_j(\tau)d\tau, \psi=(\psi_1,\dots,\psi_m)
$$

:::success
💡 Spectral Discretization imposes a **stronger prior** to the model class, based on the chosen of bases.
:::



### Tracking signals via depth–variance

Galërkin Neural ODEs trained with integral losses $l(s)= \|\beta(s)-\mathbf{z}(s)\|^2_2$ accurately recover periodic signals.



## Depth Discretization: Stacked Neural ODEs

Assume $\theta(s)$ is piesewise constant in $\mathcal{S}$, i.e. $\theta(s)=\theta_i, \forall s\in[s_i,s_{i+1}]$ and $\mathcal{S}=\bigcup^{p-1}_{i=0}[s_i,s_{i+1}]$. 

Stacked $p$ Neural ODEs with constant parameters:

$$
\mathbf{z}(S)=h_x(\mathbf{x})+\sum_{i=0}^{p-1}\int_{s_i}^{s_{i+1}}f_{\theta_i}(\tau,\mathbf{x},\mathbf{z}(\tau))d\tau
$$

Training: optimize the resulting $pn_\theta$ parameters  by

$$
\frac{d\ell}{d\theta_i}=-\int_{s_i}^{s_{i+1}}\mathbf{a}^T(\tau)\frac{\partial f_{\theta_i}}{\partial\theta_i}d\tau,\text{, where}\;\mathbf{a}(s)\text{ satisfies }\begin{cases}\dot {\mathbf{a}}^T(s)=-\mathbf{a}^T(s)\frac{\partial  f_{\theta_i}}{\partial \mathbf{z}}-\frac{\partial l}{\partial \mathbf{z}},s\in[s_i,s_{i+1}]\\ \mathbf{a}^T(S)=\frac{\partial L}{\partial \mathbf{z}(S)}\end{cases}
$$

:::success
💡 Depth Discretization allows for more **freedom**.
:::





:::info
💡 Depth–variance brings Neural ODEs closer to the ideal continuum of neural network layers with untied weights, enhancing their expressivity.
:::

# 4. Augmenting Neural ODEs

Solve the IVP in a higher dimensional space to limit the complexity of learned flows, i.e. $n_z>n_x$: 

- **0-augmentation**: initializing to 0 the $n_a\equiv n_z-n_x$ augmented dimensions: $\mathbf{z}(0)=[\mathbf{x},\mathbf{0}]$
- **IL augmentation: $h_x:\mathbb{R}^{n_x}\to\mathbb{R}^{n_z}$**

## Input-Layer Augmentation

Input network: $h_x:\mathbb{R}^{n_x}\to\mathbb{R}^{n_z}$ to compute $\mathbf{z}(0)=h_x(\mathbf{x})\ \ (4)$

- More **freedom** in determining the initial condition (instead of constraining it to concating $\mathbf{x}$ and $\mathbf{0}$)
- With small parameter cost if $h_x$ is , for example, a linear layer.
- Maintaining the structure of the first $n_x$ dimensions is important
   :arrow_right: modify $h_x$ only affect the additional $n_a$ dimension: $h_x\equiv[\mathbf{x},\xi(\mathbf{x})], \xi:\mathbb{R}^{n_x}\to\mathbb{R}^{n_a}$
    

## Higher-order Neural ODEs

### Second-order Neural ODEs

Let $\mathbf{z}(s)=[\mathbf{z}_q(s),\mathbf{z}_p(s)]$, the second-order Neural ODE: $\mathbf{\ddot{z}}_q(s)=f_{\theta(s)}(s,\mathbf{z}(s))\;\;(5)$

equivalent to the first-order system:$\begin{cases}\dot{\mathbf{z}}_q(s)=\dot{\mathbf{z}}_p(s)\\\dot{\mathbf{z}}_p(s)=f_{\theta(s)}(s,\mathbf{z}_q(s),\mathbf{z}_p(s))\end{cases}\ \ (6)$

- Number of augmented dimensions: $n_a=n_x$

### Higher-order Neural ODEs:

$$
\frac{d^n\mathbf{z}^1}{ds^n}=f_{\theta(s)}(s,\mathbf{z},\frac{d\mathbf{z}^1}{ds},\dots,\frac{d^{n-1}\mathbf{z}^1}{ds^{n-1}}),\mathbf{z}=[\mathbf{z}^1,\mathbf{z}^2,\dots,\mathbf{z}^n],\mathbf{z}^i\in\mathbb{R}^{\frac{n_z}{n}}
$$

or equavalently, $\dot{\mathbf{z}}^i=\mathbf{z}^{i+1},\dot{\mathbf{z}}^n=f_{\theta(s)}(s,\mathbf{z})$. 

- Note that $f_{\theta(s)}:\mathbb{R}^{n_z}\to\mathbb{R}^{\frac{n_z}{n}}$ instead of $\mathbb{R}^{n_z}\to\mathbb{R}^{n_z}$
    - e.g. a extension to second–order requires a number of augmented dimensions $n_z-n_x=n_a=n_x,\ n_a = n_z/2$

### **Selective higher-order augmentation**

- Allow for flexible augmentations of few dimension , $n_a<n_x$

## Revisiting results for augmented Neural ODEs

In higher dimensional state spaces, the beneﬁts of augmentation become subtle and manifest as ***performance improvements*** and a l***ower number of function evaluations (NFEs).***

The efficiency of different augmentation strategies:


# 5. Beyond Augmentation: Data-Control and Depth-Adaptation

- ANODE states that Neural ODEs cannot solve the problem with cross trajectory.
- Augmentation not always necessary
- Derieve a model to perform challenging task **without any augmentation**.

Approximation of reflection maps (such as $\varphi(x)=-x$)

## 5.1 Data-Controlled Neural ODEs

A simple handcrafted ODE capable of approximating $\varphi$ wiht arbitrary accuracy by leveraging input data $x$.

### Proposition 2

$\forall \epsilon>0, x\in\mathbb{R}$, there exists a parameter $\theta>0$ s.t. $|\varphi(x)-z(1)|<\epsilon\ \ (8)$, where $z(1)$ is the solution of the Neural ODE

$$
\begin{cases}\dot{z}(s)=-\theta(z(s)+x)\\z(0)=x\end{cases}, s\in[0,1]\ \ (9)
$$

General *data-controlled* Neural ODEs $\begin{cases}\dot{\mathbf{z}}(s)=f_{\theta(s)}(s,\mathbf{x},\mathbf{z}(s))\\\mathbf{z}(0)=h_x(\mathbf{x})\end{cases} \ \ \ (10)$

- Model (10) incorporates input data $\mathbf{x}$ into the vector field, allowing the ODE to learn a **family** of vector fields instead of a single one.
- $\mathbf{x}$ can be pass to $f_{\theta(s)}$ in different ways (e.g. an additional embedding step) 
    :arrow_right: data-control offers a natural extention to *conditonal Neural ODEs* 
    

### Data-Contral in Normalizing Flows

## 5.2 Adaptive-Depth Neural ODEs

If each input is integrated for a different depth domain, $\mathcal{S}(x)=[0,s_x^*]$


In general, we can use a hypernetwork $g$ trained to learned the integration depth of each sample. 

The adaptive depth class as the NODEs performing the mapping $\mathbf{x}\mapsto \phi_{g_w(x)}(\mathbf{x})$ , i.e. leading to 

$$
\hat{\mathbf{y}}=h_y(h_x(\mathbf{x})+\int_0^{g_x(\mathbf{x})}f_{\theta(s)}(\tau,\mathbf{x},\mathbf{z}(\tau))d\tau), \text{ where }g_x:\mathbb{R}^{n_x}\times\mathbb{R}^{n_w}\to\mathbb{R}
$$

:::success
💡 Data–control allows Neural ODEs to learn a **family of vector ﬁelds**, conditioning on input data information. Depth–adaptation **sidesteps known expressivity limitations** of continuous–depth models.
:::



## Mind your input networks

Neural ODE architectures preceded or followed by several layers of non–linear transformations  **superﬁcial evaluations** 


# 6. Conclusion

- Establish a general system–theoretic framework for Neural ODEs and dissect it into its core components.
- Depth–variance: the inﬁnite–dimensional problem linked to the true deep limit formulation of Neural ODE
    - Galerkin and piecewise–constant Neural ODEs
- Augmentation :arrow_right: more performant and parameter efﬁcient
    - input–layer and higher–order augmentation strategies
- Learning without augmentation
    - data–control and depth–adaptation

---

# — Supplementary Material —

[A. Proof and Additional Theoretical Results](https://www.notion.so/A-Proof-and-Additional-Theoretical-Results-5af21e4929e14209b0d0f89cbcfa1a32)

# B. Practical Insights for Neural ODEs

## 1. Augmentation

### Augmenting convolution and graph based architectures

- CNNs: augmenting along the ***channel*** dimension, equivalent to providing each pixel in the image additional states ([ANODEs by Dupont et al., 2019](https://arxiv.org/abs/1904.01681))
- GNNs: operating on arbitrary graphs can be achieved by augmenting each node feature with $n_a$  additional states ([GNODEs by Poli et al., 2019](https://arxiv.org/abs/1911.07532))

### Selective higher–order

Let$\mathbf{z}\equiv(\mathbf{z}_q,\mathbf{z}_p,\bar{\mathbf{z}}),\mathbf{z}_q,\mathbf{z}_p\in\mathbb{R}^{n_a/2}, \bar{\mathbf{z}}\in\mathbb{R}^{n_z-n_a}$,We can decide to give second order dynamics only to the ﬁrst  $n_a$  states while the dynamics of other $n_z − n_a$ states is free.


A similar argument could be applied to orders higher than two. Selective higher–order Neural ODEs are compatible with input layer augmentation.

## 2. Activations

### Mind your activation

The effects of appending an activation function to the last layer of $f_\theta$:

- The chosen nonlinearity will strongly affect **the “shape” of the vector ﬁeld** :arrow_right: the ﬂows learnable by the model
- Advise: append a linear layer to maximize the expressiveness of the underlying vector ﬁeld
    - priors on the desired transformation existed (e.g. boundedness) :arrow_right: nonlinearities can be desirable

### Effects of activations

- **tanh and ELU**
- **Sigmoid, ReLU or Softplus**


## 3. Regularization for Stability

FFJORD RNODE

## 4. Approximation Capabilities

- Vanilla Neural ODEs are not, in general, universal function approximators (UFAs)

Let $n_z\equiv n_x+1$ and $\mathbf{z}\equiv(\mathbf{z}_x,z_a),\;\mathbf{z}_x\in\mathbb{R}^{n_x}, z_a\in\mathbb{R}$

([Zhang et al., 2019a](https://arxiv.org/abs/1907.12998)) A depth–invariant augmented Neural ODE $\begin{bmatrix}\dot{\mathbf{z}}_x\\\dot{z}_a\end{bmatrix}=\begin{bmatrix}\mathbb{0}_{nx}\\f_\theta(\mathbf{z}_x)\end{bmatrix}$, $\begin{bmatrix}\mathbf{z}_x(0)\\z_a(0)\end{bmatrix}=\begin{bmatrix}\mathbf{x}\\0\end{bmatrix},s\in(0,1)$ where the output: $\hat{y}\equiv z_a(1)$,

can approximate any funciton $\Phi:\mathbb{R}^{n_x}\to\mathbb{R}$. ($f_\theta(x)$ is an approx. of $\Phi$)

Questions:

- Why should we use a Neural ODE if its vector ﬁeld can solve the approximation problem as
a standalone neural network?
- Can Neural ODEs be UFAs with non-UFA vector ﬁelds?

# C. Experimental Details

## 2. Experiments of Section 4

The choice of depth–invariance is motivated by the discussion carried out in Section 5: both augmentation and depth–variance can relieve approximation limitations of vanilla, depth–invariant Neural ODEs. As a result, including both renders the ablation study for augmentation strategies less accurate. We note that the results of this ablation analysis do not utilize any form of data augmentation; data augmentation can indeed be introduced to further improve performance.

## 3. Experiments of Section 5

Data–controlled Neural ODEs can be used to learn challenging maps (Dupont et al., 2019) without
augmentation.
