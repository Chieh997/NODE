# Nueral SDE: Stabilizing neural ode networks with stochastic noise
###### tags: `Paper Summary`,`Deep Learning`, `NSDEs`

---
Liu, X., Si, S., Cao, Q., Kumar, S., & Hsieh, C. J. (2019). [Neural sde: Stabilizing neural ode networks with stochastic noise.](https://arxiv.org/abs/1906.02355) *arXiv preprint arXiv:1906.02355.*

---
# 1. Introduction
+ Missing component in NODEs: **regularization** mechanism
+ Regularization of discrete network (e.g. dropout, Gaussian noise injection) are not directly applicable, because NODE network is a *deterministic system* 

**Neural Stochastic Diﬀerential Equation** (Neural SDE) network: a new continuous neural network framework which models stochastic noise injection by stochastic diﬀerential equations (SDE).

# 2. Related work
+ Neural ODEs
+ Noisy Neural Networks

# 3. Neural Stochastic Diﬀerential Equation
Neural SDE can be viewed as using randomness as a drop-in augmentation for Neural ODE, and it can include some widely used randomization layers such as dropout and Gaussian noise layer.

Toy example: $dx_t=x_tdt+\sigma x_tdB_t$
+ $\sigma =0$: $dx_t=x_tdt\implies x_t=c_0e^t$, a small perturbation in $x_t$ will be ampliﬁed through $t$.
+ $\sigma >1$: $x_t=c_0e^{(1-\sigma^2)t+\sigma B_t}\overset{a.s.}\longrightarrow 0$

![image](https://d3i71xaburhd42.cloudfront.net/98fc7a351bbb07fb3a304508e1a5ffcab03babba/4-Figure1-1.png =x300)
We can control the impact of perturbations on the output by adding a stochastic term to neural networks.

![image](https://d3i71xaburhd42.cloudfront.net/98fc7a351bbb07fb3a304508e1a5ffcab03babba/4-Figure2-1.png)

## 3.1 Modeling Randomness in Neural Networks
In Neural ODE system, a slightly perturbed input state will be ampliﬁed in deep layers which makes the system unstable to input perturbation and prone to overﬁtting. 
+ How to add randomness to garentee the robustness
+ How to solve the continuous system efficiently 

Add a singlee diffusion term into Neural ODE:
$$d\mathbf{h}_t=\mathbf{f}(\mathbf{h}_t,t;\mathbf{w})dt+\mathbf{G}(\mathbf{h}_t,t;\mathbf{v})d\mathbf{B}_t\;\;\;\;\;\;(1)$$

Note: $\mathbf{B}_{t+s}-\mathbf{B}_s$ follows Gaussian with mean 0 and variance t.

### Gaussian Noise Injection
When $\mathbf{G}(\mathbf{h}_t,t;\mathbf{v})$ is a diagonal matrix,
+ additive: $d\mathbf{h}_t=\mathbf{f}(\mathbf{h}_t,t;\mathbf{w})dt+\sum(t)d\mathbf{B}_t$
+ multiplicative:  $d\mathbf{h}_t=\mathbf{f}(\mathbf{h}_t,t;\mathbf{w})dt+\sum(\mathbf{h}_t,t)d\mathbf{B}_t$ 

vs discrete: $\mathbf{h}_{n+1}=\mathbf{h}_{n}+\mathbf{f}(\mathbf{h}_n;\mathbf{w}_n)+\sum_n\mathbf{\mathcal{z}}_n,$ with $\sum_n=\sigma_n\mathbb{I}, \mathbf{\mathcal{z}}_n\overset{i.i.d.}{\sim}\mathcal{N}(0,1)$

### Dropout
In discrete case: 
$$\mathbf{h}_{n+1}=\mathbf{h}_{n}+\mathbf{f}(\mathbf{h}_n;\mathbf{w}_n)\odot\frac{\gamma_n}{p}=\mathbf{h}_{n}+\mathbf{f}(\mathbf{h}_n;\mathbf{w}_n)+\mathbf{f}(\mathbf{h}_n;\mathbf{w}_n)\odot(\frac{\gamma_n}{p}-\mathbb{I}),$$
where $\gamma_v\overset{i.i.d}{\sim}\mathcal{B}(1,p)$
Furthermore, we have
$$\frac{\gamma_n}{p}-\mathbb{I}=\sqrt{\frac{1-p}{p}}\cdot\sqrt{\frac{p}{1-p}}(\frac{\gamma_n}{p}-\mathbb{I})\approx\sqrt{\frac{1-p}{p}}\mathbf{\mathcal{z}}_n$$
SDE with dropout:
$$d\mathbf{h}_t=\mathbf{f}(\mathbf{h}_t,t;\mathbf{w})dt+\sqrt{\frac{1-p}{p}}\mathbf{f}(\mathbf{h}_n;\mathbf{w}_n)\odot d\mathbf{B}_t$$

## 3.2 Back-propagating through SDE integral
Expected loss conidtion on the initial value: $L=\mathbb{E}[\ell(\mathbf{h}_{t_1}|\mathbf{h}_0)]$
Goal: calculate $\frac{\partial L}{\partial\mathbf{w}}$
### Theorem 3.1
For continuously diﬀerentiable loss $\ell(\mathbf{h}_{t_1})$, we can obtain an unbiased gradient estimator as
$$\widehat{\frac{\partial L}{\partial \mathbf{w}}}=\frac{\partial \ell(\mathbf{h}_{t_1})}{\partial\mathbf{w}}=\frac{\partial \ell(\mathbf{h}_{t_1})}{\partial\mathbf{h}_{t_1}}\cdot\frac{\partial\mathbf{h}_{t_1}}{\partial \mathbf{w}}\;\;\;\;(2)$$
Furthermore, if we define $\mathbf{\beta}_t=\frac{\partial \mathbf{h}_t}{\partial\mathbf{w}}$, then it follows another SDE:
$$d\mathbf{\beta}_t=(\frac{\partial\mathbf{f}(\mathbf{h}_t,t;\mathbf{w})}{\partial\mathbf{w}}+\frac{\partial\mathbf{f}(\mathbf{h}_t,t;\mathbf{w})}{\partial\mathbf{h}_t}\beta_t)dt+(\frac{\partial\mathbf{G}(\mathbf{h}_t,t;\mathbf{w})}{\partial\mathbf{w}}+\frac{\partial\mathbf{G}(\mathbf{h}_t,t;\mathbf{w})}{\partial\mathbf{h}_t}\beta_t)d\mathbf{B}_t\;\;\;\;(3)$$
It is easy to check that if $\mathbf{G}\equiv0$, then our Monte-Carlo gradient estimator (2) falls back to the exact gradient by back-propagation.


## 3.3 Roubstness of Nerual SDEs

### Assumption
1. $\mathbf{f}$ and $\mathbf{G}$ are ***at most linear*** i.e.
$\exists\;c_1>0\text{ s.t. } \lVert f(\mathbf{x},t)\rVert+\lVert G(\mathbf{x},t)\rVert\leq c_1(1-\lVert \mathbf{x}\rVert), \forall\;\mathbf{x}\in\mathbb{R}^n, t\in\mathbb{R}_+$
2. $\mathbf{f}$ and $\mathbf{G}$ are ***c2-Lipschitz*** i.e.
$\exists\;c_2>0\text{ s.t. } \lVert f(\mathbf{x},t)-f(\mathbf{y},t)\rVert+\lVert G(\mathbf{x},t)-G(\mathbf{y},t)\rVert\leq c_2\lVert\mathbf{x}-\mathbf{y}\rVert, \forall\;\mathbf{x},\mathbf{y}\in\mathbb{R}^n, t\in\mathbb{R}_+$

Base on the above assumption, we can see that ==the SDE (1) has a unique solusion==.
+ For a diffusion matrix $\mathbf{G}$ (e.g. dropout, Gaussian noise injection, etc.), both assumption are automatically satisfied as long as $\mathbf{f}$ posseses the same regularity.

Initializing Neural SDE (1) at two different values $\mathbf{h}_0$ and $\mathbf{h_0^e}=\mathbf{h}_0+\epsilon_0$, where $\epsilon_0$is the pertubation of $\mathbf{h}_0$ with $\lVert\epsilon_0\rVert\leq \delta$
$$d\mathbf{h}_t^e=\mathbf{f}(\mathbf{h}_t^e,t;\mathbf{w})dt+\mathbf{G}(\mathbf{h}_t^e,t;\mathbf{v})d\mathbf{B}'_t$$
To analyze how the perturbation evolves in the long run
$$\begin{aligned}
d\epsilon_t&=[\mathbf{f}(\mathbf{h}_t^e,t;\mathbf{w})-\mathbf{f}(\mathbf{h}_t,t;\mathbf{w})]dt+[\mathbf{G}(\mathbf{h}_t^e,t;\mathbf{v})-\mathbf{G}(\mathbf{h}_t,t;\mathbf{v})]d\mathbf{B}_t
\\&=\mathbf{f_\triangle}(\epsilon_t,t;\mathbf{w})dt+\mathbf{G_\triangle}(\epsilon_t,t;\mathbf{v})d\mathbf{B}_t\end{aligned} \;\;\;\; (4)$$
Trival solusion $\implies \epsilon_t\equiv\mathbf{0},\forall t,\mathbf{w}$
$$\mathbf{f_\triangle}(\mathbf{0},t;\mathbf{w})=\mathbf{f}(\mathbf{h}_t+\mathbf{0},t;\mathbf{w})-\mathbf{f}(\mathbf{h}_t,t;\mathbf{w})=0\\
\mathbf{G_\triangle}(\mathbf{0},t;\mathbf{v})=\mathbf{G}(\mathbf{h}_t+\mathbf{0},t;\mathbf{v})-\mathbf{G}(\mathbf{h}_t,t;\mathbf{v})=0$$
:::info
+  If we do not perturb the input data, then the output will never change. However, the solution $\epsilon_t=\mathbf{0}$ can be highly unstable.
+   By choosing the diﬀusion term $\mathbf{G}$ properly, we can always control $\epsilon_t$ within a small range.
:::


Consider SDE in (4) with initial value $\epsilon_0$:
$$
d\epsilon_t=\mathbf{f_\triangle}(\epsilon_t,t)dt+\mathbf{G_\triangle}(\epsilon_t,t)d\mathbf{B}_t \;\;\;\; (5)$$

### Lemma A.1.
If $\mathbf{f,G}$ satisfy Assumption(2), then $\mathbf{f_{\triangle},G_\triangle}$ satisfy Assumption(1)&(2).
:::spoiler
*Proof.*\
By Assumption (2) on $\mathbf{f,G}$, we can obtain that $\forall\;\epsilon,\tilde{\epsilon}\in\mathbb{R}^n,t\geq0$

$$
\begin{aligned}
\lVert\mathbf{f_\triangle}(\epsilon,t)\rVert+\lVert\mathbf{G_\triangle}(\epsilon,t)\rVert \leq c_2\lVert\epsilon\rVert \leq c_2(1+\lVert\epsilon\rVert)
\\\lVert\mathbf{f_\triangle}(\epsilon,t)-\mathbf{f_\triangle}(\tilde{\epsilon},t)\rVert+\lVert\mathbf{G_\triangle}(\epsilon,t)-\mathbf{G_\triangle}(\tilde{\epsilon},t)\rVert 
&=
\lVert(\mathbf{f}(h^e,t)-\mathbf{f}(h,t))-(\mathbf{f}(h^{\tilde{e}},t)-\mathbf{f}(h,t))\rVert+
\lVert(\mathbf{G}(h^e,t)-\mathbf{G}(h,t))-(\mathbf{G}(h^{\tilde{e}},t)-\mathbf{G}(h,t))\rVert 
\\&= 
\lVert\mathbf{f}(h^e,t)-\mathbf{f}(h^{\tilde{e}},t)\rVert+
\lVert\mathbf{G}(h^e,t)-\mathbf{G}(h^{\tilde{e}},t)\rVert
\\ &\leq
c_2\lVert\epsilon-\tilde{\epsilon}\rVert
\end{aligned}
$$
:::
:::success
With Lemma A.1, we know the SDE (5) allows a unique solution $\epsilon_t$.
:::


### Def 3.1 (Layapunov statability of SDE)
The solusion $\epsilon_t=\mathbf{0}$ of (5):
1. is ***stochastically stable*** if $\forall\alpha\in(0,1)\text{ and }r>0, \exists\;\delta=\delta(\alpha,r)>0$ s.t.
$$Pr(\lVert\epsilon_t\rVert<r,\forall\; t\geq 0)\geq 1-\alpha, \;\forall\;\lVert\epsilon_0\rVert<\delta$$ 
Moreover, it is ***stochastically asymptotically stable*** if $\forall\,\alpha\in(0,1),\exists\;\delta=\delta(\alpha)>0$ s.t 
$$Pr(\lim_{t\to\infty}\lVert\epsilon_t\rVert<0>)>1-\alpha,\;\forall\;\lVert\epsilon_0\rVert<\delta.$$

2. is ***almost surely exponentially stable*** if 
   $$\limsup_{t\to\infty}\frac{1}{t}log\lVert\epsilon_t\rVert<0, \forall \epsilon_0\in\mathbb{R}^n $$

### Lemma A.2
For (5), whenever $\epsilon_0\neq\mathbf{0}, Pr\{\epsilon_t\neq\mathbf{0},\forall t\geq0\}=1$
:::info
+ That is, almost all the sample paths starting from a non-zero initialization can **never reach zero** due to Brownian motion.
+ On the contrary, *the almost sure exponentially stability* result implies that almost all the sample paths of the solution will **be close to zero exponentially fast**.
:::


### Theorem 3.2 
If there exists a non-negative real valued function $V(\epsilon,t)\in\mathbb{R}^n\times\mathbb{R}_+$ that has continuous partial derivatives
$$V_\epsilon(\epsilon,t):=\frac{\partial V(\epsilon,t)}{\partial\epsilon},\;V_t(\epsilon,t):=\frac{\partial V(\epsilon,t)}{\partial t}, V_{\epsilon,\epsilon}(\epsilon,t):=\frac{\partial^2 V(\epsilon,t)}{\partial\epsilon\partial\epsilon^\mathsf{T}}$$
and constants $p>0,c_1>0,c_2\in\mathbb{R},c_3\geq0$ s.t. the following inequalities holds:\
$\forall \epsilon\neq \mathbf{0}, t>0$
1. $c_1\lVert\epsilon\rVert^p\leq V(\epsilon,t)$
2. $\mathcal{L}V(\epsilon,t)=V_t(\epsilon,t)+V_\epsilon(\epsilon,t)\mathbf{f}_\triangle(\epsilon,t)+\frac{1}{2}\mathsf{Tr}[\mathbf{G}_\triangle^\mathsf{T}(\epsilon,t)V_{\epsilon,\epsilon}(\epsilon,t)\mathbf{G}_\triangle(\epsilon,t)]\leq c_2V(\epsilon,t)$
3. $\lVert V_\epsilon(\epsilon,t)\mathbf{G}_\triangle(\epsilon,t)\rVert^2\geq c_3V^2(\epsilon,t)$

Then $\forall \epsilon_0\in\mathbb{R}^n$
$$\limsup_{t\to\infty}\frac{1}{t}\log\lVert\epsilon_t\rVert\leq -\frac{c_3-2c_2}{2p}\;a.s.\;\;\;(6)$$
In particular, if $c_3\geq 2c_2$, the solution $\epsilon_t\equiv\mathbf{0}$ is *almost surely exponentially stable*.

> Reference:\
> Xuerong Mao. *Stochastic diﬀerential equations and applications.* Elsevier, 2007.

We now consider a special case, when the noise is multiplicative $\mathbf{G}(\mathbf{h}_t,t)=\sigma\cdot\mathbf{h}_t$ and $m=1$. The corresponding SDE of perturbation $\epsilon_t=\mathbf{h_t^e}-\mathbf{h}_t$ has the following form 
$$d\epsilon_t=\mathbf{f_\triangle}(\epsilon_t,t)dt+\sigma\cdot\epsilon_t d\mathbf{B}_t\;\;\; (7)$$
### Corollary 3.2.1
For (7), if $\mathbf{f}(\mathbf{h}_t,t;\mathbf{w})$ is L-Lipschitz continuous w.r.t $\mathbf{h}_t$, then (7) has a unique solution with  the property:
$$\limsup_{t\to\infty}\frac{1}{t}\log\lVert\epsilon_t\rVert\leq -(\frac{\sigma^2}{2}-L)\;a.s.\;\;,\forall\epsilon_0\in\mathbb{R}^n$$
In particular, if $\sigma^2>2L$, the solution $\epsilon_t=\mathbf{0}$ is almost surely exponentially stable.

# 4. Experimental Results
![image](https://d3i71xaburhd42.cloudfront.net/98fc7a351bbb07fb3a304508e1a5ffcab03babba/4-Figure2-1.png)
+ **Neural ODE**: $\mathbf{G}(\mathbf{h}_t,t;\mathbf{v})=\mathbf{0}$
+ **Additive noise**: indepenfent of $\mathbf{h}_t$, set $\mathbf{G}(\mathbf{h}_t,t;\mathbf{v})=\sigma_t\mathbb{I}$
+ **Multiplicative noise**: proportional to $\mathbf{h}_t$, or $\mathbf{G}(\mathbf{h}_t,t;\mathbf{v})=\sigma_t\mathbf{h}_t$
+ **Dropout noise**: proportional to the drift term $\mathbf{f}$, i.e. $\mathbf{G}(\mathbf{h}_t,t;\mathbf{v})=\sigma_t\mathsf{diag}\{\mathbf{f}(\mathbf{h}_t,t;\mathbf{w})\}$


## 4.1 Generalization Performance
TTN means testing time noise.

![image](https://d3i71xaburhd42.cloudfront.net/98fc7a351bbb07fb3a304508e1a5ffcab03babba/9-Table1-1.png)
:::success
Neural SDE consistently outperforms ODE, and the reason is that adding moderate noise to the models at training time can act as a **regularizer** and thus improves testing accuracy.
:::
## 4.2 Improved non-adversarial robustness
+  Evaluating the robustness of models under non-adversarial corruptions.

![image](https://d3i71xaburhd42.cloudfront.net/98fc7a351bbb07fb3a304508e1a5ffcab03babba/10-Table2-1.png)

## 4.3 Improved adversarial robustness
Comparing the robustness against L2-norm constrained adversarial perturbations.

![image](https://d3i71xaburhd42.cloudfront.net/98fc7a351bbb07fb3a304508e1a5ffcab03babba/10-Figure3-1.png)
:::success
Both Neural SDE with multiplicative noise and dropout noise are **more resistant to adversarial attack** than Neural ODE, and dropout noise outperforms multiplicative noise.
:::
## 4.4 Visualizing the perturbations of hidden states
![image](https://d3i71xaburhd42.cloudfront.net/98fc7a351bbb07fb3a304508e1a5ffcab03babba/11-Figure4-1.png =x200)
:::success
We can observe that by adding a diﬀusion term (dropout-style noise), **the error accumulates much slower** than ordinary Neural ODE model.
:::

# 5. Conclusion
To conclude, we introduce the Neural SDE model which can ===stabilize the prediction of Neural ODE by injecting stochastic noise==. Our model can achieve better reneralization and improve the robustness to both adversarial and non-adversarial noises.
