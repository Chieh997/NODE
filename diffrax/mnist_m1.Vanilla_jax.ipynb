{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Ordinary Differential Equation on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "\n",
    "# from jaxtyping import Array, Float, Int, PyTree  # https://github.com/google/jaxtyping\n",
    "\n",
    "from utils import get_MNIST_dloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3 # learning rate for AdamW\n",
    "EPOCHS = 20\n",
    "HIDDEN_SIZE = 42     # Linear\n",
    "PRINT_EVERY = 1\n",
    "SEED = 5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader = get_MNIST_dloaders(batch_size=BATCH_SIZE, path='~/Data',download = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 28, 28)\n",
      "(64,)\n",
      "[3 9 8 9 1 1 5 9 5 0 4 8 4 1 0 6 3 7 5 0 7 7 8 8 6 2 2 3 6 1 5 8 9 0 5 0 2\n",
      " 7 6 2 3 8 9 1 4 4 8 3 8 3 7 9 2 6 0 3 6 6 6 0 9 2 6 0]\n"
     ]
    }
   ],
   "source": [
    "dummy_x, dummy_y = next(iter(trainloader))\n",
    "dummy_x = dummy_x.numpy()\n",
    "dummy_y = dummy_y.numpy()\n",
    "print(dummy_x.shape)  # 64x1x28x28\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural ODE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This could work!!!\n",
    "class Func(eqx.Module):\n",
    "    Conv1: eqx.nn.Conv2d\n",
    "    Conv2: eqx.nn.Conv2d\n",
    "    Conv3: eqx.nn.Conv2d\n",
    "    GroupNorm: eqx.nn.GroupNorm\n",
    "\n",
    "    def __init__(self, dim, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # input data = (64,1,28,28)\n",
    "        keys = jrandom.split(key, 3)\n",
    "        self.Conv1 = eqx.nn.Conv2d(1, dim, 3, padding=1, use_bias=False, key=keys[0])\n",
    "        self.GroupNorm = eqx.nn.GroupNorm(dim, dim)\n",
    "        self.Conv2 = eqx.nn.Conv2d(dim, dim, 3, padding=1, use_bias=False, key=keys[1])\n",
    "        self.Conv3 = eqx.nn.Conv2d(dim, 1, 1, key=keys[2])\n",
    "        \n",
    "    def __call__(self, t, y, args):\n",
    "        y = self.Conv1(y)\n",
    "        y = jnn.softplus(y)\n",
    "        y = self.Conv2(y)\n",
    "        y = jnn.softplus(y)\n",
    "        y = self.GroupNorm(y)\n",
    "        y = self.Conv3(y)\n",
    "        return y\n",
    "\n",
    "class Fc(eqx.Module):\n",
    "    Conv2d: eqx.nn.Conv2d\n",
    "    AdAvgPool: eqx.nn.AdaptiveAvgPool2d\n",
    "    Linear: eqx.nn.Linear\n",
    "    def __init__(self, *, key, **args):\n",
    "        keys = jrandom.split(key,2)\n",
    "        self.Conv2d = eqx.nn.Conv2d(1,1,1, key=keys[0])\n",
    "        self.AdAvgPool = eqx.nn.AdaptiveAvgPool2d(4)\n",
    "        self.Linear = eqx.nn.Linear(1*4*4, 10, key=keys[1])\n",
    "    def __call__(self, ys):\n",
    "        y = self.Conv2d(ys)\n",
    "        y = self.AdAvgPool(y)\n",
    "        y = jnp.ravel(y)\n",
    "        y = self.Linear(y)\n",
    "        return y\n",
    "\n",
    "class NeuralODE(eqx.Module):\n",
    "    func: Func\n",
    "    fc: Fc\n",
    "\n",
    "    def __init__(self, dim, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        func_key, fc_key = jrandom.split(key, 2)\n",
    "        self.func = Func(dim, key = func_key)\n",
    "        self.fc = Fc(key= fc_key)\n",
    "\n",
    "\n",
    "    def __call__(self, ts, y0):\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(self.func),\n",
    "            diffrax.Dopri5(),\n",
    "            t0=ts[0],\n",
    "            t1=ts[-1],\n",
    "            dt0=ts[1] - ts[0],\n",
    "            y0=y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=diffrax.SaveAt(ts=ts),\n",
    "        )\n",
    "        # select the last point of solution trajectory\n",
    "        return jax.vmap(self.fc, in_axes=0)(solution.ys)[-1]\n",
    "        # return solution.ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad\n",
    "def CrossEntropyLoss(model, ti, Xi, yi):\n",
    "    pred_y = jax.vmap(model, in_axes=(None, 0))(ti, Xi) \n",
    "    labels = jnn.one_hot(yi, 10)\n",
    "    _loss = optax.softmax_cross_entropy(pred_y, labels)\n",
    "    return jnp.mean(_loss)\n",
    "CrossEntropyLoss = eqx.filter_jit(CrossEntropyLoss)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(model, ti, Xi, yi):\n",
    "    pred_y = jax.vmap(model, in_axes=(None, 0))(ti, Xi)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1)\n",
    "    return jnp.mean(pred_y == yi)\n",
    "\n",
    "def evaluate(model, ti, testloader):\n",
    "    \"\"\"This function evaluates the model on the test dataset,\n",
    "    computing both the average loss and the average accuracy.\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        # Note that all the JAX operations happen inside `loss` and `compute_accuracy`,\n",
    "        # and both have JIT wrappers, so this is fast.\n",
    "        loss, _ = CrossEntropyLoss(model, ti, x, y)\n",
    "        avg_loss += loss\n",
    "        avg_acc += compute_accuracy(model, ti, x, y)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    lr=1e-3, # learning rate for AdamW\n",
    "    epochs=20,\n",
    "    hidden_size=42,     # Linear\n",
    "    seed=5678,\n",
    "    print_every=1,\n",
    "    dry_run = False, \n",
    "    model_path = None,\n",
    "    save=False\n",
    "):\n",
    "    model_key = jrandom.PRNGKey(seed)\n",
    "    iters = len(trainloader)\n",
    "    if dry_run:\n",
    "        epochs = 1\n",
    "        iters = min(int(print_every*10), iters) \n",
    "    \n",
    "    ts = jnp.linspace(0,1,2)\n",
    "\n",
    "    model = NeuralODE(dim = hidden_size, key=model_key)\n",
    "\n",
    "    optim = optax.adamw(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, Xi, yi, ti,  opt_state):\n",
    "        # Xi, yi = data_i\n",
    "        loss, grads = CrossEntropyLoss(model, ti, Xi, yi)\n",
    "        updates, opt_state = optim.update(\n",
    "            grads, opt_state, eqx.filter(model, eqx.is_array)\n",
    "        )\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "    \n",
    "    # # Loop over our training dataset as many times as we need.\n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            yield from trainloader\n",
    "            \n",
    "    # Training loop\n",
    "    # start = time.time()\n",
    "    # for epoch, (x, y) in zip(range(epochs), infinite_trainloader()):\n",
    "        # start = time.time()    \n",
    "    for epoch in range(epochs):\n",
    "        for iter, (x, y) in zip(\n",
    "            range(iters), infinite_trainloader()\n",
    "        ):\n",
    "            x = x.numpy()\n",
    "            y = y.numpy()\n",
    "            train_loss, model, opt_state = make_step(model, x, y, ts, opt_state)\n",
    "            # end = time.time()\n",
    "            if dry_run and ((iter % print_every) == 0 or (iter == iters - 1)):\n",
    "                print(f\"{iter=}, train_loss={train_loss.item():.4f}, data_shape = ({x.shape, y.shape})\")\n",
    "        if (epoch % print_every) == 0 or (epoch == epochs - 1):\n",
    "            test_loss, test_accuracy = evaluate(model, ts, testloader)\n",
    "            print(\n",
    "                f\"{epoch=}, train_loss={train_loss.item():.4f}, \"\n",
    "                f\"test_loss={test_loss.item():.4f}, test_accuracy={test_accuracy.item():.4f}\"\n",
    "                )\n",
    "            if save:\n",
    "                eqx.tree_serialise_leaves(f\"./model/NODE_epoch={epoch}.eqx\", model)\n",
    "            # print(f\"Iter: {iter}, Loss: {loss}, Computation time: {end - start}\")\n",
    "\n",
    "    return ts, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts, model = train(dry_run = True, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "2025-02-19 20:24:55.688922: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1035] Compiling 47 configs for 3 fusions on a single thread.\n",
      "2025-02-19 20:32:29.927430: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1035] Compiling 6 configs for 3 fusions on a single thread.\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "2025-02-19 20:33:50.226656: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1035] Compiling 6 configs for 3 fusions on a single thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, train_loss=0.0975, test_loss=0.2279, test_accuracy=0.9302\n",
      "epoch=1, train_loss=0.3427, test_loss=0.1841, test_accuracy=0.9421\n",
      "epoch=2, train_loss=0.2006, test_loss=0.1564, test_accuracy=0.9515\n",
      "epoch=3, train_loss=0.0645, test_loss=0.1393, test_accuracy=0.9586\n",
      "epoch=4, train_loss=0.0533, test_loss=0.1183, test_accuracy=0.9617\n",
      "epoch=5, train_loss=0.0451, test_loss=0.1032, test_accuracy=0.9691\n",
      "epoch=6, train_loss=0.1255, test_loss=0.0935, test_accuracy=0.9711\n",
      "epoch=7, train_loss=0.0678, test_loss=0.0926, test_accuracy=0.9707\n",
      "epoch=8, train_loss=0.0342, test_loss=0.1043, test_accuracy=0.9665\n",
      "epoch=9, train_loss=0.0228, test_loss=0.0809, test_accuracy=0.9753\n",
      "epoch=10, train_loss=0.0110, test_loss=0.0716, test_accuracy=0.9776\n",
      "epoch=11, train_loss=0.0672, test_loss=0.0761, test_accuracy=0.9771\n",
      "epoch=12, train_loss=0.1491, test_loss=0.0764, test_accuracy=0.9762\n",
      "epoch=13, train_loss=0.0771, test_loss=0.0666, test_accuracy=0.9790\n",
      "epoch=14, train_loss=0.0465, test_loss=0.0746, test_accuracy=0.9750\n",
      "epoch=15, train_loss=0.0233, test_loss=0.0684, test_accuracy=0.9764\n",
      "epoch=16, train_loss=0.0650, test_loss=0.0706, test_accuracy=0.9782\n",
      "epoch=17, train_loss=0.0074, test_loss=0.0605, test_accuracy=0.9790\n",
      "epoch=18, train_loss=0.0115, test_loss=0.0594, test_accuracy=0.9816\n",
      "epoch=19, train_loss=0.0234, test_loss=0.0519, test_accuracy=0.9841\n"
     ]
    }
   ],
   "source": [
    "ts, model = train(\n",
    "    lr = LEARNING_RATE, \n",
    "    epochs = EPOCHS,\n",
    "    hidden_size = HIDDEN_SIZE,     # Linear\n",
    "    seed = SEED,\n",
    "    print_every = PRINT_EVERY,\n",
    "    dry_run = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model\n",
    "# eqx.tree_serialise_leaves(f\"./model/NODE_epoch=20_250219.eqx\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_count=16,553K\n"
     ]
    }
   ],
   "source": [
    "# Number of parameters\n",
    "# Ref: https://github.com/jax-ml/jax/discussions/6153\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(eqx.filter(model, eqx.is_array)))\n",
    "print(f\"{param_count=:,.0f}K\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
