# Neural Ordinary Differential Equations
###### tags: `Paper Summary`,`Deep Learning`, `NODEs`

---
Chen, R. T., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). [Neural ordinary differential equations.](https://arxiv.org/abs/1806.07366) *arXiv preprint arXiv:1806.07366.*

Code: https://github.com/rtqichen/torchdiffeq

---

## Introduction

$\textbf{h}_{t+1}=\textbf{h}_{t}+f(\textbf{h}_{t},\theta) \;\longrightarrow\; \frac{d\textbf{h}}{dt}=f(\textbf{h}_{t},t,\theta)$
![image](https://i.imgur.com/L0zMtkQ.png =800x)

A new family of deep neural network models
RNN: Discrete sequence of hidden layers
NODE: parameterize the derivative of the hidden state (continuous-depth models)
pros:
+ constant memory cos
+  adapt their evaluation strategy to each input
+ explicitly trade numerical precision for speed
generative model


----


## Discrete vs Continuous
<font color="#707070">hidden state:</font>
### $\textbf{h}_{t+1}=\textbf{h}_{t}+f(\textbf{h}_{t},\theta)$
<font color="#C0C0C0">ResNet, RNN, NF</font>

RNN structure

----

## Solve ODE: [Euler's Method](https://en.wikipedia.org/wiki/Euler_method)

$\begin{align}
& t_{n+1} = t_{n}+h\\
& \textbf{z}(t_{n+1}) = \textbf{z}(t_n)+hf(\textbf{z}(t_n),t_n)
\end{align}$

![Euler's Method](https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Euler_method.svg/1280px-Euler_method.svg.png =320x250)

initial point slope
RNN is a special case for h = 1

----

## Discrete vs Continuous
<font color="#707070">hidden state:</font>
### $\textbf{h}_{t+1}=\textbf{h}_{t}+f(\textbf{h}_{t},\theta)$
<font color="#C0C0C0">ResNet, RNN, NF</font>
#### <span><!-- .element: class="fragment" data-fragment-index="1" -->$\textbf{h}(t+\delta)=\textbf{h}(t)+\delta\frac{d\textbf{h}}{dt}\;\longleftrightarrow\; \frac{d\textbf{h}}{dt}\approx\frac{\textbf{h}(t+\delta)-\textbf{h}(t)}{\delta}$</span>

### <span><!-- .element: class="fragment" data-fragment-index="1" -->$\frac{d\textbf{h}}{dt}=f(\textbf{h}_{t},t,\theta)$</span>
<span><!-- .element: class="fragment" data-fragment-index="1" --><font color="#C0C0C0">ODEs</font></span>

take smaller h
View as definiiton of deriative
NN >> solving ODE


---

## Forword Propagation

$\begin{align}
& \frac{d\textbf{z}}{dt}=f(\textbf{z}_{t},t,\theta)\\
& \textbf{z}(t_1)=\textbf{z}(t_0)+\int^{t_1}_{t_0}f(\textbf{z}(t_0),t,\theta)=\text{ODEsolve}(\textbf{z}(t_0),f,t_0,t_1)\\
& \text{Loss: } L(\textbf{z}(t_1)) \longrightarrow \frac{\partial L}{\partial\theta}\end{align}$

Tranform our problem in to NN form
t0: input, t1: output
ODE sovlver: Euler/ RK4
function of \theta and update \theta
gradient of theta >>  gradient of the loss depends on the hidden state [Adjoint state]

----

## Solve ODE: [Runge-Kutta Method](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods)

$\begin{align}
& t_{n+1} = t_{n}+h\\\\
& \;\; k_1 = f(\textbf{z}(t_n),t_n)\\
& \;\; k_2 = f(\textbf{z}(t_n)+\frac{h}{2}k_1,t_n+\frac{h}{2})\\
& \;\; k_3 = f(\textbf{z}(t_n)+\frac{h}{2}k_2,t_n+\frac{h}{2})\\
& \;\; k_4 = f(\textbf{z}(t_n)+hk_3,t_n+h)\\\\
& \textbf{z}(t_{n+1}) = \textbf{z}(t_n)+\frac{h}{6}(k_1+2k_2+2k_3+k_4)
\end{align}$

Commonly used Numerical ODE Solver: RK4

----

## Solve ODE: [Runge-Kutta Method](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods)
$\textbf{z}(t_{n+1}) = \textbf{z}(t_n)+\frac{h}{6}(k_1+2k_2+2k_3+k_4)$
![R-K](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Runge-Kutta_slopes.svg/1024px-Runge-Kutta_slopes.svg.png =500x500)

Initial, mid, end point >> slope >> average
Could we do better?

---

## Forword Propagation

$\begin{align}
& \frac{d\textbf{z}}{dt}=f(\textbf{z}_{t},t,\theta)\\
& \textbf{z}(t_1)=\textbf{z}(t_0)+\int^{t_1}_{t_0}f(\textbf{z}(t_0),t,\theta)=\text{ODEsolve}(\textbf{z}(t_0),f,t_0,t_1)\\
& \text{Loss: } L(\textbf{z}(t_1)) \longrightarrow \frac{\partial L}{\partial\theta}\end{align}$


<span><!-- .element: class="fragment" data-fragment-index="1" -->$\text{Adjoint state: } \textbf{a}(t)=\frac{\partial L}{\partial \textbf{z}(t)}$</span>


----

## Adjoint Sesitive Method

$\text{Adjoint state: } \textbf{a}(t)=\frac{\partial L}{\partial \textbf{z}(t)}; \frac{d\textbf{a}(t)}{dt}= -\textbf{a}(t)\frac{\partial f(\textbf{z}(t),t,\theta)}{\partial \textbf{z}(t)}$

![adjoint](https://img-blog.csdnimg.cn/20190909191745862.png =600x400)

deriatve of adjoint state(chain rule) 
ODE solver run backward t1 to t0

??
Reverse-mode differentiation of an ODE solution. The adjoint sensitivity method solves an augmented ODE backwards in time. The augmented system contains both the original state and the sensitivity of the loss with respect to the state. If the loss depends directly on the state at multiple observation times, the adjoint state must be updated in the direction of the partial derivative of the loss with respect to each observation.
??

----

## Backward Propagation
$\begin{align}
\textbf{z}(t_0)& =\text{ODEsolve}(\textbf{z}(t_1),f(\textbf{z}(t_1),t,\theta),t_1,t_0)\\
\textbf{a}(t_0)=\frac{\partial L}{\partial \textbf{z}(t_0)}& =\text{ODEsolve}(\frac{\partial L}{\partial \textbf{z}(t_1)}, -\textbf{a}(t)^T\frac{\partial f}{\partial \textbf{z}(t)},t_1,t_0)\\
\frac{\partial L}{\partial\theta}& =\text{ODEsolve}(\textbf{0}_{|\theta|}, -\textbf{a}(t)^T\frac{\partial f}{\partial\theta},t_1,t_0)
\end{align}$

$\implies\begin{bmatrix}\textbf{z}(t_0)\\
\frac{\partial L}{\partial \textbf{z}(t_0)}\\
\frac{\partial L}{\partial\theta}\end{bmatrix}
=\text{ODEsolve}(
\begin{bmatrix}\textbf{z}(t_1)\\
\frac{\partial L}{\partial \textbf{z}(t_1)}\\
\textbf{0}_{|\theta|}\end{bmatrix},
\begin{bmatrix}f(\textbf{z}(t_1),t,\theta)\\
-\textbf{a}(t)^T\frac{\partial f}{\partial \textbf{z}(t)}\\
-\textbf{a}(t)^T\frac{\partial f}{\partial\theta}\end{bmatrix},
t_1, t_0)$

Algorithm
gradient of \theta
matrix form

---

## Supervised Learning
![SupLearnModel](https://i.imgur.com/WuiZwxx.png =1024x)

number of function evaluations (implicit number of layers) 
ODE-Nets and RK-Nets can achieve around the same performance

----

![SupLearnNFE](https://i.imgur.com/MD6EyNv.png)
+  Tuning the tolerance gives us a trade-off between accuracy and computational cost.
+ The number of evaluations in the backward pass is roughly half of the forward pass.

+ a,b: trade off
+ c: the number of evaluations in the backward pass is roughly half of the forward pass >> adjoint method: computationally efﬁcient
+ d:  the ‘depth‘ of an ODE solution >> the number of evaluations 
    + increases throughout training >>  to increasing complexity of the model.

---

## Continuous Normalizing Flow

![NF](https://i.imgur.com/virau8O.png)

+ Change of variables: $p_\textbf{X}(\textbf{x})=p_\textbf{Z}(f(\textbf{x}))\lvert\det Df(\textbf{x})\rvert$

+ Normalizing Flow: $\textbf{z}_1=f(\textbf{z}_0)\implies \log p(\textbf{z}_1)=\log p(\textbf{z}_0)-\log\lvert\det\frac{\partial f}{\partial\textbf{z}_0}\rvert$


<span><!-- .element: class="fragment" data-fragment-index="1" -->Instantaneous Change of Variables: $\frac{\partial\log p(\textbf{z}(t))}{\partial t}=-tr(\frac{df}{d\textbf{z}(t)})$</span>

----

## Continuous Normalizing Flow

$f(\textbf{z}(t),t)\implies \frac{d\textbf{z}}{dt}=\sum_n\sigma_n(t)f_n(\textbf{z})$

+ Density matching: $\min \text{KL}(q(\textbf{x})||p(\textbf{x}))$

![DensityMatching](https://i.imgur.com/De79GXZ.png)

----

## Continuous Normalizing Flow

+ CNF  can compute the reverse transformation for about the same cost as the forward pass
+ Maximum Likelihood Training: $\max \mathbb{E}_{p(\textbf{x})}[\log q(\textbf{x})]$

![MLE](https://i.imgur.com/P9HQg9X.png)

---

## A generative latent function time-series model
 
![GenLatentTime](https://i.imgur.com/1Y1iP2M.png)

----

![RNNvsNODE](https://i.imgur.com/8Xh6Ju0.png =600x)![Traj](https://i.imgur.com/IZTsISE.png =300x)
![accuracy](https://i.imgur.com/IfHWnW9.png =400x)

---

## Conclusion
<!-- .slide: style="font-size: 30px;" -->
+ **Memory Efficiency**
    <font color="#C0C0C0">Not storing any intermediate quantities of the forward pass allows us to train our models with constant memory cost as a function of depth.</font>
+ **Adaptive Computation**
    <font color="#C0C0C0">Allow explicit control of the tradeoff between computation speed and accuracy.</font>
+ **Scalable and Invertible NF**
    <font color="#C0C0C0">Construct a new class of invertible density models that can be trained directly by maximum likelihood.</font>
+ **Continuous Time-series Model**
    <font color="#C0C0C0">Naturally incorporate data which arrives at arbitrary times.</font>

----

## Reference

+ R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud, ["Neural Ordinary Differential Equations,"](https://proceedings.neurips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html) *Neural Information Processing Systems*, 2018. 
+ V. Voleti, ["A brief tutorial on Neural ODEs,"](https://voletiv.github.io/docs/presentations/20200710_Mila_Neural_ODEs_tutorial_Vikram_Voleti.pdf) Mila, Montreal, Canada, 2020.



